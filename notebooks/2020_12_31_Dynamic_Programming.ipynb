{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming\n",
    "This chapter is a bit of an oddity to me because I have always been taught to think of dynamic programming as a matter of decomposition and memoization, which it turns out does play into the history of optimal control...  The thing that throws me for a loop here is that I usually think of dynamic programming as an algorithmic strategy that you apply for some problems with the right structure.  I guess the structure is simply that you can express the optimal solution recursively, in terms of the optimal solution to a smaller problem, and that is certainly the gist of the Bellman equations, so there's that... \n",
    "\n",
    "Two quick notes about DP approaches to RL.  First, note that these algorithms require the dynamics, which is not a factor in the more usual DP algorithms you see in intro algo courses.  Second, we very often don't have the dynamics in real problems, but the approaches used in those situations often try to approximate these algorithms.  So it's worth understanding them!\n",
    "\n",
    "Anyhow, this chapter starts to get into how to find optimal policies for _finite MDPs_ (i.e., finite state, reward, and action spaces) when we have a good model of the dynamics $p(s', r | s, a)$.  The basic idea is to use the Bellman optimality equations.  As a refresher, here they are again: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "v_*(s) & = \\max_a \\mathbb{E}[R_{t+1} + \\gamma v_*(S_{t+1}) | S_t = s, A_t = a ] \\\\\n",
    "       & = \\max_a \\sum_{s', r} p(s', r|s,a) [r + \\gamma v_*(s)] \\\\\n",
    "q_*(s,a) & = \\mathbb{E} \\left[ R_{t+1} + \\gamma max_{a'} q_*(S_{t+1}, a' | S_t = s, A_t = a \\right] \\\\       \n",
    "         & = \\sum_{s', r} p(s', r|s,a) [r + \\gamma max_{a'} q_*(s,a')]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Finding an optimal policy (the _control_ problem) requires that we have a way of figuring out how good a given policy is (the _evaluation_ problem), which is basically estimating the value function.  So that's where we start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Policy Evaluation\n",
    "First off, it seems reasonable that in order to improve on a given policy, $\\pi$, we need a way to evaluate how good it is.  This is called _policy evaluation_, or the _prediction problem_.  Note that we are not claiming that $\\pi$ is optimal - we are just asking - how do we evaluate the value of an arbitrary $\\pi$?  Recall one form of the Bellman equation for state value functions: \n",
    "\n",
    "$$\n",
    "v_{\\pi}(s) = \\sum_a \\pi(a|s) \\sum_{s', r} p(s',r|s,a) [r + \\gamma v_{\\pi}(s')]\n",
    "$$\n",
    "\n",
    "First off, we note that $v_{\\pi}$ exists and is unique as long as $\\gamma < 1$ or termination is guaranteed from all states under the policy.  In principle, the above relation defines a linear system of equations we could solve but that is tedious and in any event this kind of approach simply won't scale for bigger problems.  So we focus on iterative approaches.  We start with a initial guess at $v_{\\pi}, v_0$, and we improve on it (make it closer to the actual value function) iteratively using the update: \n",
    "\n",
    "For each state:\n",
    "$$\n",
    "\\begin{align}\n",
    "v_{k+1}(s) & = \\mathbb{E}_{\\pi} [ R_{t+1} + \\gamma v_k(S_{t+1}) | S_t = s ] \\\\\n",
    "           & = \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r|s,a) [r + \\gamma v_k(s)] \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "That is, we simply use the Bellman equation as an update.  Under the same conditions that guarantee the existence and uniqueness of $v_{\\pi}$, we can prove that the sequence of $v_k$ converges to $v_{\\pi}$ - we can definitely see that the latter is a fixed point of these updates, so this certainly seems reasonable!  Note - these kinds of updates are called _expected updates_ because we are calculating them using the expectation of the next state and reward rather than a single experiment or sample of the next state and reward...  \n",
    "\n",
    "### How we do this in practice\n",
    "First, analogous to Gibbs sampling, this is most often done _in place_.  Say we are representing $v_k$ as an array with an element for each state $s$.  The most straight forward way to run this is to start with such a vector for iteration $k$, and then for iteration $k+1$ we write into a new array representing $v_{k+1}$.  In practice, however, we converge faster if we keep a single array (i.e., we update value in place) so that later updates get the benefit of the improved estimates for earlier states.  We also need some way to decide when we've effectively converged - we can just test whether the maximum of $\\max_s v_{k+1}(s) - v_k(s) < \\epsilon$  for some threshold small $\\epsilon$.  If it is, we stop.\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 4.3</b>  What are the analogous equations for the action-value function $q_{\\pi}$?\n",
    "\n",
    "For each state and action pair, we update:\n",
    "$$\n",
    "q_{k+1}(s,a) = \\sum_{s',r} p(s',r|s,a) \\left[ r + \\gamma \\sum_{a'} \\pi(a'|s') q_k(s',a') \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improvement\n",
    "So now that we can evaluate a policy, how do we actually improve on a given policy $\\pi$?  Specifically, say we are considering changing $\\pi$ to deterministically select action $a$ in state $s$.  The value of doing this is given simply by the current action-value function (assume we have used policy evaluation to get $v_{\\pi}$): \n",
    "\n",
    "$$\n",
    "q_{\\pi}(s,a) = \\sum_{s',r} p(s',r|s,a) [r + \\gamma v_{\\pi}(s')]\n",
    "$$\n",
    "\n",
    "If this is greater than $v_{\\pi}(s)$, then, because of the Markov property, it is reasonable to assume that we should do action $a$ every time we are in state $s$, so we should update $\\pi$ to do $a$ when in $s$.  This is the basic idea behind the _policy improvement theorem_ : let $\\pi$ and $\\pi'$ be any pair of deterministic policies, with: \n",
    "\n",
    "$$\n",
    "q_{\\pi}(s, \\pi'(s)) \\ge v_{\\pi}(s)\n",
    "$$\n",
    "\n",
    "This is just saying the same thing as the above, but jumping straight to the relevant comparison.  Then $\\pi'$ is as good as or better than $\\pi$: \n",
    "\n",
    "$$\n",
    "v_{\\pi'}(s) \\ge v_{\\pi}(s) \\text{  } \\forall s \\in \\mathcal{S}\n",
    "$$\n",
    "\n",
    "The proof of this is simple - see page 78.  \n",
    "\n",
    "So what happens if we expand this to consider changes to all states and all possible actions?  That is, for each state, update $\\pi \\rightarrow \\pi'$ to select the action $a$ in a greedy fashion? \n",
    "\n",
    "For each state, we update the policy:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\pi(s) & = \\underset{a}{argmax} q_{\\pi}(s,a) \\\\\n",
    "       & = \\underset{a}{argmax} \\mathbb{E}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) | S_t = s, A_t = a ] \\\\\n",
    "       & = \\underset{a}{argmax} \\sum_{s',r} p(s',r|s,a) [r + \\gamma v_{\\pi}(s)]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "Now let's say that we do alternating rounds of policy evaluation and improvement until we are no longer making any updates.  Then we know that the Bellman optimality condition holds, since it certainly holds for any given state by construction.  Thus, when we have converged, we should have an optimal value function and policy!  This algorithm is called _policy iteration_.\n",
    "\n",
    "When I first read this, I was a bit confused - won't this immediately give you the optimal policy?  But no, it won't - recall that this procedure is for a given policy $\\pi$, and we are updating that each time we update $\\pi$ (we have to do another round of policy evaluation for each iteration of policy improvement).  The process looks like this.  Say we start with an initial policy $\\pi_0$...  We do iterations of policy evaluation and improvement: \n",
    "\n",
    "$$\n",
    "\\pi_0 \\overset{E}{\\rightarrow} v_{\\pi_0} \\overset{I}{\\rightarrow} \\pi_1 \\overset{E}{\\rightarrow} v_{\\pi_1} \\overset{I}{\\rightarrow} \\pi_2 \\overset{E}{\\rightarrow} v_{\\pi_2} \\overset{I}{\\rightarrow} \\pi_3 \\dots \\overset{I}{\\rightarrow} \\pi_* \\overset{E}{\\rightarrow} v_{\\pi_*}\n",
    "$$\n",
    "\n",
    "After each round of policy improvement, we've improved the policy but still only with respect to the current policy - we don't necessarily have the optimal policy!  So we have to keep going until we are no longer updating the value function or policy, in which case we have converged per the discussion above.\n",
    "\n",
    "Another way of describing this is that each policy improvement step makes the policy greedy with respect to the value function of the current policy; by the policy improvement theorem, this is a good step.  Then we need to estimate the value function for the new policy, and around we go until we have a policy that is already greedy with respect to its value function, at which point the Bellman optimality condition holds by definition and we are done.\n",
    "\n",
    "Finally, note that this was all written with respect to deterministic policies, but in fact holds for stochastic policies as well - just assign zero probability to all non-optimal actions in the update. Each round of policy iteration improves on the previous policy, and since we have a finite MDP this will eventually converge.\n",
    "\n",
    "### Policy Iteration\n",
    "```\n",
    "Initialize pi[s] for all s, prev_policy_value = None\n",
    "while True:\n",
    "\n",
    "    # Policy evaluation\n",
    "    def eval_policy(pi, epsilon=1e-6):\n",
    "        Initialize V[s] for all states s\n",
    "        while True:\n",
    "            delta = 0\n",
    "            sum_V = 0\n",
    "            for each state s:\n",
    "                v = V[s]\n",
    "                V[s] = sum over s',r of p(s',r|s,pi[s])*(r + gamma*V[s'])\n",
    "                delta = max(delta, |v - V(s)|)\n",
    "                sum_V += V[s]\n",
    "            if delta < epsilon: break\n",
    "        return V, V_sum\n",
    "\n",
    "    V, current_policy_value = eval_policy(pi)\n",
    "    if current_policy_value == prev_policy_value:\n",
    "        return V, pi\n",
    "    else:\n",
    "        prev_policy_value = current_policy_value\n",
    "    \n",
    "    # Policy Improvement\n",
    "    stop_condition = True\n",
    "    for each state s: \n",
    "        old_action = pi[s]\n",
    "        pi[s] = argmax over a of (sum over s', r of p(s',r|s,a)*(r + gamma*V[s']))\n",
    "        if old_action != pi[s]:\n",
    "            stop_condition = False\n",
    "    if stop_condition:\n",
    "        return V, pi # These are close to optimal now\n",
    "    \n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.4\n",
    "The policy iteration algorithm on page 80 has a subtle bug in that it may never terminate if the policy continually switches between two or more policies that are equally good. This is ok for pedagogy, but not for actual use. Modify the pseudocode so that convergence is guaranteed.\n",
    "\n",
    "See above pseudo code block.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# >>> Exercise 4.5  <<<\n",
    "How would policy iteration be defined for action values? Give a complete algorithm for computing $q_*$, analogous to that on page 80 for computing $v_*$.\n",
    "\n",
    "Here is the skeleton.  I don't quite understand how this should be very different from the policy iteration algorithm.  Certainly if you ran policy iteration and arrived at an optimal value function and policy it would be trivial to get an estimate for $q_*(s,a)$ - you could, sweep over states and actions and just calculate the expected value of the actions from different states, plus the discounted valued of the expected next state given $v_*(S_{k+1})$... \n",
    "\n",
    "Okay, so I think the only reasonable squaring of the circle is to say that instead of estimating V given pi, we are intended to estimate Q given pi, and then use that Q to improve on the policy.  So that's pretty straight forward:\n",
    "\n",
    "```\n",
    "Initialize q[s,a] for all s, a.  \n",
    "Initialize pi[s] for all s\n",
    "Initialize prev_policy_value = None\n",
    "while True:\n",
    "\n",
    "    # Policy evaluation - this uses q[s,a] instead of pi?  \n",
    "    def eval_policy(pi, epsilon=1e-6):\n",
    "        Initialize Q[s,a] for all states s and actions a.\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for each state s:\n",
    "                for each action a:\n",
    "                    q = Q[s,a]\n",
    "                    Q[s,a] = sum over s',r of p(s',r|s,a)*(r + gamma*(sum over a' of pi[s',a']*Q[s',a']))\n",
    "                delta = max(delta, |q - Q[s,a]|)\n",
    "            if delta < epsilon: break\n",
    "        return Q\n",
    "\n",
    "    Q = eval_policy(pi)\n",
    "    \n",
    "    # Policy Improvement\n",
    "    stop_condition = True\n",
    "    for each state s: \n",
    "        old_action = pi[s]\n",
    "        pi[s] = argmax over a of Q[s,a]\n",
    "        if old_action != pi[s]:\n",
    "            stop_condition = False\n",
    "    if stop_condition:\n",
    "        return Q and pi\n",
    "    \n",
    "```   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.6\n",
    "Suppose you are restricted to considering only policies that are $\\epsilon$-soft, meaning that the probability of selecting each action in each state, $s$, is at least $\\epsilon/|\\mathcal{A}(s)|$. Describe qualitatively the changes that would be required in each of the steps 3, 2, and 1, in that order, of the policy iteration algorithm for $v_*$ on page 80.\n",
    "\n",
    "Step 3: Basically, instead of putting all the probability on the argmax action, we need to spread it out so each action receives at least $\\epsilon / |\\mathcal{A}(s)|$ mass.  \n",
    "\n",
    "Step 2: We now have a stochastic policy so the evaluation routine needs to assign V[s] to be the expectation over the actions from state s instead of assuming the policy is deterministic.\n",
    "\n",
    "Step 1: The initialization of the policy needs to change to ensure the starting policy adheres to our constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "The above algorithm must do policy evaluation every iteration, which is itself a potentially quite expensive iterative process.  It turns out that we can just truncate it after one iteration and combine the update with policy improvement - the resulting algorithm is called _value iteration_:\n",
    "\n",
    "For each state, we update: \n",
    "$$\n",
    "v_{k+1}(s) = \\max_a \\sum_{s',r} p(s',r|s,a)[ r + \\gamma v_k(s') ]\n",
    "$$\n",
    "\n",
    "Note that unlike the policy evaluation iterations, we are not doing an expectation over actions under policy $\\pi$ - instead we are taking the max, effectively updating $v$ while also trying to improve the policy.  This converges to $v_*$ under the same conditions that guarantee the existence of $v_*$.  As for policy evaluation, we stop when $v$ changes very little over all states.\n",
    "\n",
    "### Value Iteration\n",
    "```\n",
    "def value_iteration(S, epsilon):\n",
    "    Initialize V[s] for all s in S, except V[terminal] = 0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in S:\n",
    "            v = V[s]\n",
    "            V[s] = max over a of (sum over s', r of (p(s',r|s,a)(r + gamma*V[s'])))\n",
    "            delta = max(delta, |v - V(s)|)\n",
    "        if delta < epsilon: \n",
    "            break\n",
    "    return a deterministic policy pi based on V[s], which should be a good \n",
    "    approximation to the optimal value function...\n",
    "    \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises 4.8-9\n",
    "<b>Exercise 4.8</b> There is nothing to be gained from winning more than 100, so it doesn't pay to ever bet more than 50.  That way, if you lose on that bet, then you can start from something again. \n",
    "\n",
    "<b>Exercise 4.9</b>  Implement value iteration for the gambler's problem and solve for $p_h = 0.25$ and $p_h = 0.55$.  Show results graphically as in figure 4.3.  \n",
    "\n",
    "See below... \n",
    "\n",
    "<b>Exercise 4.10</b> What is the analog of the value iteration update (4.10) for action values $q_{k+1}(s,a)$?\n",
    "\n",
    "$$\n",
    "q_{k+1}(s,a) = \\sum_{s',r} p(s',r|s,a) [r + \\gamma \\sum_{a'} \\pi(a'|s') q_k(s',a')]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous Dynamic Programming\n",
    "Each iteration of policy evaluation and policy improvement as described above requires a full sweep through all states $s \\in \\mathcal{S}$, which can be very expensive.  It often works well to update values of for only some subset of states per iteration.  Indeed, some states may have their values updated multiple times before others are updated once.  However, in order to converge, we must eventually get around to updating all states of course.  At an extreme, we can update the value of only a single state at a time!  This sort of game is called asynchronous dynamic programming, and is very useful for online learning situations where we are in a single state IRL!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Policy Iteration\n",
    "There is a lot of room between our two extremes of iterations of policy-evalution followed policy improvement vs value iteration.  For instance, one can often get fast convergence by doing multiple rounds of policy evaluation for each policy improvement sweep.  GPI is an umbrella term for algorithms that alternate policy evaluation (not always full - see value iteration!) and policy improvement, and includes almost all RL algorithms in practice.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate DP\n",
    "There is a cool video about applications of DP to fleet management, where the state space is really huge and exact DP is out of the question!  See Warren Powell's page <a href='https://castlelab.princeton.edu/jungle/'>here</a> for some very cool stuff!  An interesting question is how you can improve on these algorithms and their robustness when you have real time sensors (to bring things back to what we do at work...).  Also, how do things change for automated vehicles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
